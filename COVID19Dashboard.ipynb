{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COVID19Dashboard.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "tSRO_pcEKKl3"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNcOxxj+/yGL9K5hpi8dZ2q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yy20190404/db_covid/blob/master/COVID19Dashboard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fKPrxkIWtsQ",
        "colab_type": "text"
      },
      "source": [
        "# COVID19-Dashboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bKkl797W-2A",
        "colab_type": "text"
      },
      "source": [
        "## Collect COVID19 data from web"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74_fubMipVjL",
        "colab_type": "text"
      },
      "source": [
        "### install python libraries, make directories\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSaqamHNqG3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m pip install --upgrade pip\n",
        "!pip install requests\n",
        "!python -m pip install requests-html\n",
        "!python -m pip install seaborn\n",
        "!pip install japanize-matplotlib\n",
        "!python -m pip install flask\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "!mkdir templates\n",
        "!mkdir static\n",
        "!mkdir static/img\n",
        "!mkdir static/csv\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "!cp drive/'My Drive'/'Colab Notebooks'/japan-map-master/japan-map.jquery.json templates/\n",
        "!cp drive/'My Drive'/'Colab Notebooks'/japan-map-master/jquery.japan-map.js templates/\n",
        "!cp drive/'My Drive'/'Colab Notebooks'/japan-map-master/jquery.japan-map.min.js templates/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDlACgyHXIQ6",
        "colab_type": "text"
      },
      "source": [
        "### Reshape data to use dashboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KNjh_FTv3X0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dldata = \"\"\"\n",
        "\n",
        "#!/usr/bin/env python\n",
        "#-*- coding:utf-8 -*-\n",
        "\n",
        "import datetime\n",
        "import gc\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import requests\n",
        "from requests_html import HTMLSession\n",
        "\n",
        "def get_converted_multi_columns(df, just_second=False, to_snake_case=True):\n",
        "  if just_second:\n",
        "    return [col[1] for col in df.columns.values]\n",
        "  else:\n",
        "    if to_snake_case:\n",
        "        return [col[0] + '_' + col[1] for col in df.columns.values]\n",
        "    else:\n",
        "        return [col[0] + col[1].capitalize() for col in df.columns.values]\n",
        "\n",
        "def down_load():\n",
        "  HEADERS_DIC = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36\"}\n",
        "  URLWW1 = \"https://data.humdata.org/hxlproxy/api/data-preview.csv?url=https%3A%2F%2Fraw.githubusercontent.com%2FCSSEGISandData%2FCOVID-19%2Fmaster%2Fcsse_covid_19_data%2Fcsse_covid_19_time_series%2Ftime_series_covid19_confirmed_global.csv&filename=time_series_covid19_confirmed_global.csv\"\n",
        "  URLWW2 = \"https://data.humdata.org/hxlproxy/api/data-preview.csv?url=https%3A%2F%2Fraw.githubusercontent.com%2FCSSEGISandData%2FCOVID-19%2Fmaster%2Fcsse_covid_19_data%2Fcsse_covid_19_time_series%2Ftime_series_covid19_deaths_global.csv&filename=time_series_covid19_deaths_global.csv\"\n",
        "  URLWW3 = \"https://data.humdata.org/hxlproxy/api/data-preview.csv?url=https%3A%2F%2Fraw.githubusercontent.com%2FCSSEGISandData%2FCOVID-19%2Fmaster%2Fcsse_covid_19_data%2Fcsse_covid_19_time_series%2Ftime_series_covid19_recovered_global.csv&filename=time_series_covid19_recovered_global.csv\"\n",
        "  URLJP  = \"https://dl.dropboxusercontent.com/s/6mztoeb6xf78g5w/COVID-19.csv\"\n",
        "  SAVE_NAMES = [\"covid19_ww_confirmed_global.csv\", \"covid19_ww_deaths_global.csv\",\"covid19_ww_recovered_global.csv\", \"covid19_jp.csv\"]\n",
        "  DIRCSV = \"./static/csv/\"\n",
        "  urls = [URLWW1, URLWW2, URLWW3, URLJP]\n",
        "  del_cols = [\"Province/State\", \"Lat\", \"Long\"]\n",
        "  \n",
        "  ##############################################################################\n",
        "  ## Reshape of worldwide COVID19 daily numbers csv\n",
        "  ##############################################################################\n",
        "  today = datetime.date.today()\n",
        "  save_names = SAVE_NAMES.copy()\n",
        "  del save_names[-1]\n",
        "  i = 0\n",
        "  for f_name in save_names:\n",
        "    # Download a target file if it is not exist\n",
        "    if os.path.isfile(DIRCSV + f_name) == False:\n",
        "      r = requests.get(urls[i], headers=HEADERS_DIC)\n",
        "      with open(DIRCSV + f_name, mode='w', encoding='utf_8') as f:\n",
        "        f.write(r.text)\n",
        "\n",
        "      ## Reshape csv file\n",
        "      df = pd.read_csv(DIRCSV + f_name)\n",
        "      ### Drop unnessesary columns\n",
        "      df = df.drop(del_cols, axis=1) \n",
        "      ### Reshape date style          \n",
        "      cols = df.columns\n",
        "      dates = []\n",
        "      for col in cols:\n",
        "        if col == 'Country/Region':\n",
        "          dates.append(col)\n",
        "        else:\n",
        "          day = datetime.datetime.strptime(col, '%m/%d/%y')  \n",
        "          dates.append(day.strftime('%Y-%m-%d'))\n",
        "      df.columns = dates\n",
        "      ### Group and sum each country\n",
        "      df = df.groupby(\"Country/Region\").sum()\n",
        "      ### Save as csv file\n",
        "      df.to_csv(DIRCSV + f_name, encoding='utf_8_sig')\n",
        "      ### Delete dataframe instance\n",
        "      del df\n",
        "    else:\n",
        "      ### Get file datetime\n",
        "      dt = os.path.getmtime(DIRCSV + f_name)\n",
        "      dt = datetime.datetime.fromtimestamp(dt)\n",
        "      dt = dt.strftime('%Y-%m-%d')\n",
        "      ## Download and reshape csv file when the exist file not made on today\n",
        "      if str(dt) != str(today):\n",
        "        r = requests.get(urls[i], headers=HEADERS_DIC)\n",
        "        with open(DIRCSV + f_name, mode='w', encoding='utf_8') as f:\n",
        "          f.write(r.text)\n",
        "        df = pd.read_csv(DIRCSV + f_name)\n",
        "        df = df.drop(del_cols, axis=1) \n",
        "        cols = df.columns\n",
        "        dates = []\n",
        "        for col in cols:\n",
        "          if col == 'Country/Region':\n",
        "            dates.append(col)\n",
        "          else:\n",
        "            day = datetime.datetime.strptime(col, '%m/%d/%y')  \n",
        "            dates.append(day.strftime('%Y-%m-%d'))\n",
        "        df.columns = dates\n",
        "        ### Group and sum each countory\n",
        "        df = df.groupby(\"Country/Region\").sum()\n",
        "        df = df.set_index('Country/Region')\n",
        "        ### Save as csv file\n",
        "        df.to_csv(DIRCSV + f_name, encoding='utf_8_sig')\n",
        "        ### Delete dataframe instance\n",
        "        del df\n",
        "    i += 1\n",
        "\n",
        "  ##############################################################################\n",
        "  ## Reshape of Japanese COVID19 daily numbers csv\n",
        "  ##############################################################################\n",
        "  f_name = SAVE_NAMES[3]\n",
        "  today = datetime.date.today()\n",
        "  # Download a target file if it is not exist\n",
        "  if os.path.isfile(DIRCSV + f_name) == False:\n",
        "    r = requests.get(urls[3], headers=HEADERS_DIC)\n",
        "    with open(DIRCSV + f_name, mode='w', encoding='utf_8') as f:\n",
        "      f.write(r.text)\n",
        "  else:\n",
        "    dt = os.path.getmtime(DIRCSV + f_name)\n",
        "    dt = datetime.datetime.fromtimestamp(dt)\n",
        "    dt = dt.strftime('%Y-%m-%d')\n",
        "    if today != dt:\n",
        "      r = requests.get(urls[3], headers=HEADERS_DIC)\n",
        "      with open(DIRCSV + f_name, mode='w', encoding='utf_8') as f:\n",
        "        f.write(r.text)\n",
        "  ## Reshape csv file\n",
        "  df = pd.read_csv(DIRCSV + f_name)\n",
        "  \n",
        "  ################################################################################\n",
        "  ## Download of Japanese COVID19 data\n",
        "  ## Reshape the data\n",
        "  ################################################################################\n",
        "  cols = df.columns\n",
        "  df = df.loc[:, [\"通し\", \"受診都道府県\", \"確定日\", \"更新日時\", \"年代\", \"性別\", \"ステータス\", \"人数\"]]\n",
        "  df.columns = [\"No.\", \"Prefecture\", \"Date\", \"Saved_Date\", \"Genelation\", \"Sex\", \"Status\", \"Number\"]\n",
        "  df['Status'] = df['Status'].replace(np.nan, \"confirmed\")\n",
        "  df['Status'] = df['Status'].replace('退院', \"recovered\")\n",
        "  df['Status'] = df['Status'].replace('死(.*)', \"death\", regex=True)\n",
        "  df['Prefecture'] = df['Prefecture'].replace('中部国際空港', '愛知県')\n",
        "  df['Prefecture'] = df['Prefecture'].replace('成田空港', '千葉県')\n",
        "  df['Prefecture'] = df['Prefecture'].replace('羽田空港', '東京都')\n",
        "  df['Prefecture'] = df['Prefecture'].replace('関西国際空港', '大阪府')\n",
        "  sr = df['Saved_Date']\n",
        "  modified_date = str(sr[0])\n",
        "  modified_date = datetime.datetime.strptime(modified_date, '%m/%d/%Y %H:%M')\n",
        "  modified_date = modified_date.strftime('%Y-%m-%d %H:%M')\n",
        "  df.at[0, 'Saved_Date'] = modified_date\n",
        "  df = df.dropna(subset=[\"Date\"])\n",
        "  dates = []\n",
        "  for date in df['Date']:\n",
        "    day = datetime.datetime.strptime(str(date), '%m/%d/%Y')\n",
        "    dates.append(day.strftime('%Y-%m-%d'))\n",
        "  df['Date'] = dates\n",
        "  df.to_csv(DIRCSV + \"covid19_jp_all.csv\", encoding='utf_8_sig')\n",
        "\n",
        "  ## 日本の都道府県別・日付別発症者数リスト\n",
        "  dfc = df.drop([\"No.\", \"Saved_Date\", \"Genelation\", \"Sex\", \"Status\"], axis=1)\n",
        "  dfc = dfc.groupby([\"Prefecture\", \"Date\"]).sum().unstack()\n",
        "  dfc = dfc.replace(np.nan, 0.0)\n",
        "  dfc.columns = get_converted_multi_columns(dfc, just_second=True)\n",
        "  dfc.to_csv(DIRCSV + \"covid19_jp_prf_cfm.csv\", encoding='utf_8_sig')\n",
        "\n",
        "  ## 日本の都道府県別・日付別死者数リスト\n",
        "  dfc = df.drop([\"No.\", \"Saved_Date\", \"Genelation\", \"Sex\", \"Number\"], axis=1)\n",
        "  dfc = dfc[dfc[\"Status\"] != \"confirmed\"]\n",
        "  dfc = dfc[dfc[\"Status\"] != \"recovered\"]\n",
        "  dfc[\"Status\"] = 1\n",
        "  dfc = dfc.groupby([\"Prefecture\", \"Date\"]).sum().unstack()\n",
        "  dfc = dfc.replace(np.nan, 0.0)\n",
        "  dfc.columns = get_converted_multi_columns(dfc, just_second=True)\n",
        "  dfc.to_csv(DIRCSV + \"covid19_jp_prf_dth.csv\", encoding='utf_8_sig')\n",
        "  \n",
        "  ## 日本の都道府県別・日付別退院者数リスト\n",
        "  dfc = df.drop([\"No.\", \"Saved_Date\", \"Genelation\", \"Sex\", \"Number\"], axis=1)\n",
        "  dfc = dfc[dfc[\"Status\"] != \"confirmed\"]\n",
        "  dfc = dfc[dfc[\"Status\"] != \"death\"]\n",
        "  dfc[\"Status\"] = 1\n",
        "  dfc = dfc.groupby([\"Prefecture\", \"Date\"]).sum().unstack()\n",
        "  dfc = dfc.replace(np.nan, 0.0)\n",
        "  dfc.columns = get_converted_multi_columns(dfc, just_second=True)\n",
        "  dfc.to_csv(DIRCSV + \"covid19_jp_prf_rcv.csv\", encoding='utf_8_sig')\n",
        "\n",
        "  ## 日本の都道府県別・ 男性数リスト\n",
        "  dfc = df.drop([\"No.\", \"Saved_Date\", \"Genelation\", \"Status\", \"Number\"], axis=1)\n",
        "  dfc = dfc[dfc[\"Sex\"] != \"女性\"]\n",
        "  dfc[\"Sex\"] = 1\n",
        "  dfc = dfc.groupby([\"Prefecture\", \"Date\"]).sum().unstack()\n",
        "  dfc = dfc.replace(np.nan, 0.0)\n",
        "  dfc.columns = get_converted_multi_columns(dfc, just_second=True)\n",
        "  dfc.to_csv(DIRCSV + \"covid19_jp_prf_male.csv\", encoding='utf_8_sig')\n",
        "\n",
        "  ## 日本の都道府県別・ 女性数リスト\n",
        "  dfc = df.drop([\"No.\", \"Saved_Date\", \"Genelation\", \"Status\", \"Number\"], axis=1)\n",
        "  dfc = dfc[dfc[\"Sex\"] != \"男性\"]\n",
        "  dfc[\"Sex\"] = 1\n",
        "  dfc = dfc.groupby([\"Prefecture\", \"Date\"]).sum().unstack()\n",
        "  dfc = dfc.replace(np.nan, 0.0)\n",
        "  dfc.columns = get_converted_multi_columns(dfc, just_second=True)\n",
        "  dfc.to_csv(DIRCSV + \"covid19_jp_prf_female.csv\", encoding='utf_8_sig')\n",
        "\n",
        "  ## 日本の都道府県別・ 年代別リスト\n",
        "  dfc = df.drop([\"No.\", \"Date\", \"Saved_Date\", \"Sex\", \"Status\"], axis=1)\n",
        "  dfc = dfc.replace(np.nan, 99.0)\n",
        "  dfc = dfc.replace(\"0-10\", 1.0)\n",
        "  dfc = dfc.replace(\"10\", 10.0)\n",
        "  dfc = dfc.replace(\"20\", 20.0)\n",
        "  dfc = dfc.replace(\"30\", 30.0)\n",
        "  dfc = dfc.replace(\"40\", 40.0)\n",
        "  dfc = dfc.replace(\"50\", 50.0)\n",
        "  dfc = dfc.replace(\"60\", 60.0)\n",
        "  dfc = dfc.replace(\"70\", 70.0)\n",
        "  dfc = dfc.replace(\"80\", 80.0)\n",
        "  dfc = dfc.replace(\"90\", 90.0)\n",
        "  dfc = dfc.replace(10, 10.0)\n",
        "  dfc = dfc.replace(20, 20.0)\n",
        "  dfc = dfc.replace(30, 30.0)\n",
        "  dfc = dfc.replace(40, 40.0)\n",
        "  dfc = dfc.replace(50, 50.0)\n",
        "  dfc = dfc.replace(60, 60.0)\n",
        "  dfc = dfc.replace(70, 70.0)\n",
        "  dfc = dfc.replace(80, 80.0)\n",
        "  dfc = dfc.replace(90, 90.0)\n",
        "  dfc[\"Genelation\"] = dfc[\"Genelation\"].replace(\"不明\", 99.0)\n",
        "  dfc[\"Genelation\"] = dfc[\"Genelation\"].astype(int)\n",
        "  dfc = dfc.groupby([\"Prefecture\", \"Genelation\"]).sum().unstack()\n",
        "  dfc.columns = get_converted_multi_columns(dfc, just_second=True)\n",
        "  dfc = dfc.replace(np.nan, 0.0)\n",
        "  dfc = dfc.astype(int)\n",
        "  dfc.to_csv(DIRCSV + \"covid19_jp_prf_gen.csv\", encoding='utf_8_sig')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  down_load()\n",
        "\"\"\"\n",
        "\n",
        "  \n",
        "with open('dldata.py', mode='w', encoding='utf_8') as f:\n",
        "  f.write(dldata)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2SCT8AoXkat",
        "colab_type": "text"
      },
      "source": [
        "### Make files to use dashboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2PciUKScU_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install plotly\n",
        "!pip install dash\n",
        "!pip install dash-html-components\n",
        "!pip install dash-core-components\n",
        "!pip install dash-table\n",
        "!pip install dash-daq \n",
        "!pip install dash_bootstrap_components\n",
        "!pip install gunicorn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbm2SpV0X8Ed",
        "colab_type": "text"
      },
      "source": [
        "## Dashboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWWIhqUyB2U4",
        "colab_type": "text"
      },
      "source": [
        "### Build internal server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7lRxIVEkbw5",
        "colab_type": "text"
      },
      "source": [
        "### app.pyの作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q50bHWo_jCQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dashfile = \"\"\"\n",
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "import datetime\n",
        "import dash\n",
        "import dash_core_components as dcc\n",
        "import dash_html_components as html\n",
        "from dash.dependencies import Input, Output\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import dldata\n",
        "\n",
        "dldata.down_load()\n",
        "\n",
        "\n",
        "DIRCSV = 'static/csv/'\n",
        "CSV_FILES = ('covid19_ww_confirmed_global.csv',\n",
        "            'covid19_ww_deaths_global.csv',\n",
        "            'covid19_ww_recovered_global.csv',\n",
        "            'covid19_jp_prf_cfm.csv',\n",
        "            'covid19_jp_prf_dth.csv',\n",
        "            'covid19_jp_prf_rcv.csv',\n",
        "            'covid19_jp_prf_male.csv',\n",
        "            'covid19_jp_prf_female.csv',\n",
        "            'covid19_jp_prf_gen.csv')\n",
        "\n",
        "#df0 = pd.read_csv(DIRCSV + CSV_FILES[0])\n",
        "df0 = pd.read_csv(DIRCSV + CSV_FILES[0], header=0, index_col=0)\n",
        "#df1 = pd.read_csv(DIRCSV + CSV_FILES[1], index_col=0)\n",
        "#df2 = pd.read_csv(DIRCSV + CSV_FILES[2], index_col=0)\n",
        "#df3 = pd.read_csv(DIRCSV + CSV_FILES[3], index_col=0)\n",
        "#df4 = pd.read_csv(DIRCSV + CSV_FILES[4], index_col=0)\n",
        "#df5 = pd.read_csv(DIRCSV + CSV_FILES[5], index_col=0)\n",
        "#df6 = pd.read_csv(DIRCSV + CSV_FILES[6], index_col=0)\n",
        "#df7 = pd.read_csv(DIRCSV + CSV_FILES[7], index_col=0)\n",
        "#df8 = pd.read_csv(DIRCSV + CSV_FILES[8], index_col=0)\n",
        "\n",
        "\n",
        "#df0 = df0.set_index('Country/Region')\n",
        "df0.loc['Worldwide'] = df0.sum()\n",
        "df0_x = df0.columns.tolist()\n",
        "df0_y = df0.loc['Worldwide', ].tolist()\n",
        "df0_z = df0.index.tolist()\n",
        "dict0 = []\n",
        "for x in df0_z:\n",
        "  dict0.append({'label': x, 'value': x})\n",
        "\n",
        "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
        "\n",
        "app = dash.Dash(__name__, external_stylesheets=external_stylesheets)\n",
        "server = app.server\n",
        "\n",
        "# 背景色と文字色の設定\n",
        "colors = {\n",
        "    'background': '#111111',\n",
        "    'text': '#7FDBFF'\n",
        "}\n",
        "\n",
        "app.layout = html.Div(style={'backgroundColor': colors['background']}, children=[\n",
        "    html.H1(\n",
        "      children='COVID19ダッシュボード',\n",
        "      style={'textAlign': 'center',\n",
        "            'color': colors['text']\n",
        "      }\n",
        "    ),\n",
        "\n",
        "    html.Div(\n",
        "      children='COVID19ワールドワイド及び日本国内感染者数情報', \n",
        "      style={'textAlign': 'center',\n",
        "        'color': colors['text']\n",
        "      }\n",
        "    ),\n",
        "\n",
        "    dcc.Dropdown(\n",
        "      id='my_ticker_symbol',\n",
        "      options=dict0,\n",
        "      value='Worldwide',\n",
        "    ),\n",
        "\n",
        "    dcc.Graph(\n",
        "        id='example-graph',\n",
        "        figure={\n",
        "            'data':[\n",
        "              {'x': df0_x, 'y': df0_y}\n",
        "            ],\n",
        "            'layout': {\n",
        "                'plot_bgcolor': colors['background'],\n",
        "                'paper_bgcolor': colors['background'],\n",
        "                'font': {\n",
        "                    'color': colors['text']\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    )\n",
        "])\n",
        "\n",
        "@app.callback(\n",
        "  Output('example-graph', 'figure'),\n",
        "  [Input('my_ticker_symbol', 'value')])\n",
        "\n",
        "def update_graph(stock_ticker):\n",
        "  df0_y = df0.loc[stock_ticker, ].tolist()\n",
        "  fig = {\n",
        "    'data':[\n",
        "      {'x': df0_x, 'y': df0_y}\n",
        "    ],\n",
        "    'layout': {\n",
        "      'title': stock_ticker,\n",
        "      'plot_bgcolor': colors['background'],\n",
        "      'paper_bgcolor': colors['background'],\n",
        "      'font': {'color': colors['text']}\n",
        "    }\n",
        "  }\n",
        "  return fig\n",
        "\n",
        "\n",
        "#if __name__ == '__main__':\n",
        "  #app.run_server(debug=True)\n",
        "\"\"\"\n",
        "\n",
        "with open('app.py', mode='w', encoding='utf_8') as f:\n",
        "  f.write(dashfile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVI1wtOg4BqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "guni = \"\"\"\n",
        "GUNICORN_CMD_ARGS=\"--bind=127.0.0.1:4040 --workers=3\" gunicorn app:app\n",
        "\"\"\"\n",
        "\n",
        "with open('guni.py', mode='w', encoding='utf-8') as f:\n",
        "  f.write(guni)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hrq8CAJ8Ri2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python app.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Won7cu4lkvHG",
        "colab_type": "text"
      },
      "source": [
        "### ngrok tonneling on Colaboratory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMXNNayP9Ke7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_ipython().system_raw('./ngrok http 8000 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "  \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n",
        "\n",
        "#!python app.py\n",
        "! gunicorn app:server\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idmKjYB9uBvD",
        "colab_type": "text"
      },
      "source": [
        "# Files to use Heroku"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYqzFWkW-YN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rt = \"\"\"\n",
        "Python 3.6.9\n",
        "\"\"\"\n",
        "\n",
        "with open('runtime.txt', mode='w', encoding='utf_8') as f:\n",
        "  f.write(rt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpynilp4t56D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "procf = \"\"\"\n",
        "web: gunicorn app:server\n",
        "\"\"\"\n",
        "\n",
        "with open('procfile', mode='w', encoding='utf_8') as f:\n",
        "  f.write(procf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRrvNWin8URj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "require = \"\"\"\n",
        "numpy\n",
        "pandas\n",
        "requests\n",
        "requests-html\n",
        "seaborn\n",
        "flask\n",
        "plotly\n",
        "dash\n",
        "dash-html-components\n",
        "dash-core-components\n",
        "dash-table\n",
        "dash-daq \n",
        "dash_bootstrap_components\n",
        "gunicorn\n",
        "\"\"\"\n",
        "\n",
        "with open('requirements.txt', mode='w', encoding='utf_8') as f:\n",
        "  f.write(require)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}